Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NIr_BIxafr4H2cB1g9IN-GQcndaGa7-h

# 1. Setup for Google Colab
!pip install selenium
!apt-get update
!apt install chromium-chromedriver -y
!cp /usr/lib/chromium-browser/chromedriver /usr/bin

import sys
sys.path.insert(0, '/usr/lib/chromium-browser/chromedriver')

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
import time
import pandas as pd

# 2. Configure Selenium
options = Options()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
driver = webdriver.Chrome(options=options)

# 3. Get car URLs from ALL pages
def get_all_car_urls():
    car_urls = []
    page = 1
    while True:
        print(f"üîÑ Collecting car URLs from page {page}...")
        url = f"https://motorwatt.com/ev-marketplace/electric-cars?start={(page - 1) * 24}"
        driver.get(url)
        time.sleep(3)

        car_cards = driver.find_elements(By.CSS_SELECTOR, "a.el-item")
        if not car_cards:
            break  # no more pages

        for card in car_cards:
            try:
                link = card.get_attribute("href")
                if link:
                    car_urls.append(link)
            except Exception as e:
                print(f"Error extracting URL: {e}")
        page += 1

    return car_urls

# 4. Scrape car details from a single page
def scrape_car_page(car_url):
    local_driver = webdriver.Chrome(options=options)
    local_driver.get(car_url)
    time.sleep(2)

    try:
        def safe_text(selector, by=By.CSS_SELECTOR):
            try:
                return local_driver.find_element(by, selector).text.strip()
            except:
                return None

        car_name = safe_text("h3.el-title")
        country = safe_text("div.el-meta")
        price = safe_text("span.price_val")

        def get_ul_value(label):
            try:
                items = local_driver.find_elements(By.CSS_SELECTOR, "ul.uk-list li.el-item")
                for item in items:
                    title_span = item.find_element(By.CLASS_NAME, "el-title").text.strip().lower()
                    if label in title_span:
                        return item.find_element(By.CLASS_NAME, "el-content").text.strip()
            except:
                return None
            return None

        producer = get_ul_value("e-car producer")
        registration = get_ul_value("first registration")
        charge_rate = get_ul_value("1 charge range")

        def detail_block(label):
            try:
                return local_driver.find_element(By.XPATH, f"//div[text()='{label}']/../following-sibling::div//div").text.strip()
            except:
                return None

        mileage = detail_block("current milage:")
        max_speed = detail_block("max speed (km/h):")
        battery = detail_block("battery (kWh):")
        engine = detail_block("engine capacity (p.h.):")

        detailed_title = safe_text("h2.uk-h4.uk-heading-bullet")
        seller_location = safe_text("div.uk-panel.uk-text-small.uk-text-emphasis.uk-margin-small")
        phone_number = safe_text("div.el-content.uk-panel.uk-h4")

        return {
            "Detailed Car Title": detailed_title,
            "Seller Location": seller_location,
            "Phone Number": phone_number,
            "E-car Producer": producer,
            "First Registration": registration,
            "Charge Rate": charge_rate,
            "Price ($)": price,
            "Current Mileage": mileage,
            "Max Speed (km/h)": max_speed,
            "Battery (kWh)": battery,
            "Engine Capacity (p.h.)": engine,
            "Car URL": local_driver.current_url
        }

    finally:
        local_driver.quit()

# 5. Run scraping for all pages
car_urls = get_all_car_urls()
print(f"‚úÖ Found {len(car_urls)} car URLs in total.")

all_data = []
for i, url in enumerate(car_urls):
    print(f"üîç Scraping car {i+1}/{len(car_urls)}...")
    try:
        data = scrape_car_page(url)
        all_data.append(data)
    except Exception as e:
        print(f"‚ùå Error scraping {url}: {e}")

# 6. Save the results
df = pd.DataFrame(all_data)
df.to_csv("/content/electric_cars_all_pages.csv", index=False)
print("‚úÖ Data saved to /content/electric_cars_all_pages.csv")

# Show first rows
df.head()
from google.colab import files
files.download("/content/electric_cars_all_pages.csv")

